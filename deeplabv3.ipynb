{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project D - Severstal Steel Defect Detection Kaggle Competition \n",
    "## Semantic Segmentation of steel defects\n",
    "### Authors: Utkrisht Rajkumar, Subrato Chakravorty, Chi-Hsin Lo\n",
    "\n",
    "This is the file used to train and benchmark DeepLabV3+. We tested this model with a width 0.1x the width of the original model by modifying **alpha** in the call to Deeplabv3() as this moderates the width of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.layers import concatenate, Input\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import Callback, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.utils import multi_gpu_model\n",
    "import keras\n",
    "from model import Deeplabv3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')\n",
    "train_df['ImageId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n",
    "train_df['ClassId'] = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[1])\n",
    "train_df['hasMask'] = ~ train_df['EncodedPixels'].isna()\n",
    "mask_count_df = train_df.groupby('ImageId').agg(np.sum).reset_index()\n",
    "mask_count_df.sort_values('hasMask', ascending=False, inplace=True)\n",
    "non_missing_train_idx = mask_count_df[mask_count_df['hasMask'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "def mask2rle(img):\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle2mask(rle, input_shape):\n",
    "    width, height = input_shape[:2]\n",
    "    mask= np.zeros( width*height ).astype(np.uint8)\n",
    "    array = np.asarray([int(x) for x in rle.split()])\n",
    "    starts = array[0::2]\n",
    "    lengths = array[1::2]\n",
    "    current_position = 0\n",
    "    for index, start in enumerate(starts):\n",
    "        mask[int(start):int(start+lengths[index])] = 1\n",
    "        current_position += lengths[index]\n",
    "    return mask.reshape(height, width).T\n",
    "\n",
    "def build_masks(rles, input_shape):\n",
    "    depth = len(rles)\n",
    "    masks = np.zeros((*input_shape, depth))\n",
    "    for i, rle in enumerate(rles):\n",
    "        if type(rle) is str:\n",
    "            masks[:, :, i] = rle2mask(rle, input_shape)\n",
    "    return masks\n",
    "\n",
    "def build_rles(masks):\n",
    "    width, height, depth = masks.shape\n",
    "    rles = [mask2rle(masks[:, :, i]) for i in range(depth)]\n",
    "    return rles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, df, target_df=None, mode='fit',\n",
    "                 base_path='../input/train_images',\n",
    "                 batch_size=32, dim=(256, 1600), n_channels=1,\n",
    "                 n_classes=4, random_state=2019, shuffle=True):\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.df = df\n",
    "        self.mode = mode\n",
    "        self.base_path = base_path\n",
    "        self.target_df = target_df\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_batch = [self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        X = self.__generate_X(list_IDs_batch)\n",
    "        \n",
    "        if self.mode == 'fit':\n",
    "            y = self.__generate_y(list_IDs_batch)\n",
    "            return X, y\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            return X\n",
    "\n",
    "        else:\n",
    "            raise AttributeError('The mode parameter should be set to \"fit\" or \"predict\".')\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.seed(self.random_state)\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __generate_X(self, list_IDs_batch):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_batch):\n",
    "            im_name = self.df['ImageId'].iloc[ID]\n",
    "            img_path = f\"{self.base_path}/{im_name}\"\n",
    "            img = self.__load_grayscale(img_path)\n",
    "            \n",
    "            # Store samples\n",
    "            X[i,] = img\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def __generate_y(self, list_IDs_batch):\n",
    "        y = np.empty((self.batch_size, *self.dim, self.n_classes), dtype=int)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_batch):\n",
    "            im_name = self.df['ImageId'].iloc[ID]\n",
    "            image_df = self.target_df[self.target_df['ImageId'] == im_name]\n",
    "            \n",
    "            rles = image_df['EncodedPixels'].values\n",
    "            masks = build_masks(rles, input_shape=self.dim)\n",
    "            \n",
    "            y[i, ] = masks\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def __load_grayscale(self, img_path):\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = img.astype(np.float32) / 255.\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train = np.array(non_missing_train_idx.index)\n",
    "\n",
    "np.random.shuffle(train)\n",
    "train_num = int(len(train)*0.95)\n",
    "train_idx = train[:train_num]\n",
    "val_idx = train[train_num:]\n",
    "\n",
    "train_generator = DataGenerator(train_idx, df=mask_count_df, target_df=train_df, batch_size=BATCH_SIZE, n_classes=4)\n",
    "val_generator = DataGenerator(val_idx, df=mask_count_df, target_df=train_df, batch_size=BATCH_SIZE, n_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import binary_crossentropy\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true,y_pred):\n",
    "    return 1-dice_coef(y_true,y_pred)\n",
    "\n",
    "def BCE_loss(y_true, y_pred):\n",
    "    return (binary_crossentropy(y_true, y_pred))\n",
    "\n",
    "def bce_dice(y_true, y_pred):\n",
    "    return BCE_loss(y_true, y_pred) + dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeplab_model = Deeplabv3(input_shape=(256, 1600, 1), alpha =0.1, classes=4) \n",
    "deeplab_model.compile(optimizer='adam', loss=bce_dice, metrics=[dice_coef])\n",
    "checkpoint = ModelCheckpoint('./models/deeplabv1.h5', monitor='val_dice_coef', verbose=0, \n",
    "                             save_best_only=True, save_weights_only=False, mode='auto')\n",
    "deeplab_history = deeplab_model.fit_generator(train_generator, validation_data=val_generator, verbose=0,\n",
    "                              callbacks=[checkpoint], use_multiprocessing=False, workers=1, epochs=20)\n",
    "pd.DataFrame(deeplab_history.history).to_csv('./history/deeplab_1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
